{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Assignment Answers (Theory Section)\n",
        "1. What is a parameter?\n",
        "- In machine learning, a parameter is a configuration variable that is internal to the model and whose value can be estimated from data. These are the values that the learning algorithm learns during training, such as the weights and biases in a neural network. They define the model's predictive capability.\n",
        "2. What is correlation?\n",
        "- Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It indicates both the strength and direction of the linear relationship between the variables, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation).\n",
        "3. What does negative correlation mean?\n",
        "- Negative correlation means that as one variable increases, the other variable tends to decrease. For example, in many scenarios, as the temperature outside decreases, heating bill costs tend to increase. The correlation coefficient for a negative correlation will be between -1 and 0.\n",
        "4. Define Machine Learning. What are the main components in Machine Learning?\n",
        "- Machine Learning is a field of artificial intelligence where systems learn from data to identify patterns, make decisions, or predict outcomes without being explicitly programmed. The main components typically include data (for training and testing), a model (the algorithm that learns), features (the input variables), and an objective function/loss function (to evaluate model performance).\n",
        "5. How does loss value help in determining whether the model is good or not?\n",
        "- The loss value (or error) quantifies how poorly a model performs on a given dataset. A lower loss value generally indicates a better-performing model, as it means the model's predictions are closer to the actual values. During training, the goal is often to minimize this loss value.\n",
        "6. What are continuous and categorical variables?\n",
        "- Continuous variables are numerical variables that can take any value within a given range, often involving decimals (e.g., temperature, height, price). Categorical variables, on the other hand, represent types or categories and can only take on a limited number of discrete values (e.g., gender, city, product type).\n",
        "7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "- Categorical variables need to be converted into a numerical format before being fed into most machine learning models. Common techniques include One-Hot Encoding (creates new binary columns for each category) and Label Encoding (assigns a unique integer to each category).\n",
        "8. What do you mean by training and testing a dataset?\n",
        "- Training a dataset involves using a portion of the data to teach the machine learning model to learn patterns and relationships. Testing a dataset involves using a separate, unseen portion of the data to evaluate how well the trained model generalizes to new data and to assess its performance.\n",
        "9. What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module within the scikit-learn library that provides a collection of functions and classes for data preprocessing tasks. These tasks are crucial for transforming raw data into a format suitable for machine learning algorithms, such as scaling, encoding categorical variables, and imputation.\n",
        "10. What is a Test set?\n",
        "- A test set is a subset of your original dataset that is held out from the training process. Its purpose is to provide an unbiased evaluation of the final model's performance on unseen data. It helps in assessing how well the model generalizes and avoids overfitting.\n",
        "11. How do we split data for model fitting (training and testing) in Python?\n",
        "- In Python, we typically use the train_test_split function from scikit-learn's model_selection module. This function randomly divides the dataset into training and testing subsets, ensuring that both sets are representative of the original data.\n",
        "12. How do you approach a Machine Learning problem?\n",
        "- Approaching a machine learning problem usually involves several steps: understanding the problem and data, data collection, data preprocessing (cleaning, transformation), exploratory data analysis (EDA), feature engineering, model selection, model training, model evaluation, hyperparameter tuning, and finally, deployment.\n",
        "13. Why do we have to perform EDA before fitting a model to the data?\n",
        "- Exploratory Data Analysis (EDA) is crucial before model fitting because it helps in understanding the dataset's characteristics, identifying patterns, detecting outliers, and uncovering relationships between variables. This insight guides preprocessing steps, feature engineering, and model selection, ultimately leading to a more robust and accurate model.\n",
        "14. What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation means that one event is the direct result of another event. Correlation, as discussed, only indicates that two variables move together. The key difference is that correlation does not imply causation. For example, ice cream sales and shark attacks might be positively correlated (both increase in summer), but ice cream sales don't cause shark attacks; the causal factor is warm weather, which drives both.\n",
        "15. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- An optimizer is an algorithm or function that adjusts the parameters of a machine learning model (like weights and biases) during training to minimize the loss function. Its goal is to find the optimal set of parameters that results in the best model performance.\n",
        "Different types of optimizers include:\n",
        "Gradient Descent (GD): Updates parameters by taking steps proportional to the negative of the gradient of the loss function. It can be slow on large datasets.\n",
        "Example: Imagine finding the lowest point in a valley by always taking a small step downhill.\n",
        "Stochastic Gradient Descent (SGD): Similar to GD but updates parameters after evaluating the gradient for each training example. This makes it faster and helps escape local minima but can be noisy.\n",
        "Example: Taking a step downhill after looking at only one patch of ground.\n",
        "Mini-Batch Gradient Descent: A compromise between GD and SGD, updating parameters after processing a small \"batch\" of training examples. This balances speed and stability.\n",
        "Example: Taking a step downhill after looking at a small group of patches of ground.\n",
        "Adam (Adaptive Moment Estimation): A more advanced optimizer that uses adaptive learning rates for each parameter, combining ideas from other optimizers. It's often a good default choice for many deep learning tasks.\n",
        "Example: Finding the lowest point in a valley, but smartly adjusting your step size for different terrains based on past observations.\n",
        "16. What is sklearn.linear_model?\n",
        "- sklearn.linear_model is a module within the scikit-learn library that provides a wide range of algorithms for linear models. This includes models for regression (like Linear Regression, Ridge, Lasso) and classification (like Logistic Regression, Perceptron).\n",
        "17. What does model.fit() do? What arguments must be given?\n",
        "- model.fit() is a method used to train a machine learning model. It takes the training data and corresponding target values, allowing the model to learn the underlying patterns and relationships between features and the target. The primary arguments required are X (the training features) and y (the target variable).\n",
        "18. What does model.predict() do? What arguments must be given?\n",
        "- model.predict() is a method used to make predictions using a trained machine learning model. After the model has learned from the training data, predict() takes new, unseen input features and outputs the model's estimated target values. The main argument required is X_new (the features of the data for which you want predictions).\n",
        "19. What are continuous and categorical variables?\n",
        "\n",
        "- Continuous variables are numerical values that can be measured along a continuum and often have infinite possibilities between any two values (e.g., age, height, temperature). Categorical variables, in contrast, represent distinct groups or labels and can't be measured on a continuous scale (e.g., colors, types of fruit, yes/no answers).\n",
        "20. What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature scaling is a data preprocessing technique used to standardize the range of independent variables or features of the data. It helps in machine learning by preventing features with larger numerical ranges from dominating features with smaller ranges during model training, especially for algorithms sensitive to feature magnitudes (e.g., K-Nearest Neighbors, Support Vector Machines, Gradient Descent).\n",
        "21. How do we perform scaling in Python?\n",
        "- In Python, scaling is typically performed using transformers from scikit-learn's sklearn.preprocessing module. Common scalers include StandardScaler (which standardizes features by removing the mean and scaling to unit variance) and MinMaxScaler (which scales features to a fixed range, usually 0 to 1).\n",
        "22. What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a vital module in the scikit-learn library dedicated to transforming raw data into a suitable format for machine learning algorithms. It offers a diverse set of tools for tasks like standardizing, normalizing, encoding categorical data, and imputing missing values, which are essential for model performance and stability.\n",
        "23. How do we split data for model fitting (training and testing) in Python?\n",
        "- In Python, the most common and robust way to split data is by using train_test_split from sklearn.model_selection. This function randomly partitions the dataset into two subsets (training and testing) while maintaining the original data's distribution (especially important with the stratify parameter for classification).\n",
        "24. Explain data encoding?\n",
        "- Data encoding is the process of converting data from one format or representation to another, often from categorical to numerical. This is a critical step in machine learning because most algorithms require numerical input. Encoding schemes like One-Hot Encoding and Label Encoding translate non-numerical labels into numerical values that algorithms can process.\n"
      ],
      "metadata": {
        "id": "7X-Z0oGETUK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Assignment Answers (Coding Section)**"
      ],
      "metadata": {
        "id": "mQbMF1EAULdl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2bUJ-siTS9t",
        "outputId": "201e02e9-dd41-4eeb-dc69-7406110035b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape (X): (150, 4)\n",
            "Original data shape (y): (150,)\n",
            "\n",
            "Shape of X_train: (105, 4)\n",
            "Shape of X_test: (45, 4)\n",
            "Shape of y_train: (105,)\n",
            "Shape of y_test: (45,)\n"
          ]
        }
      ],
      "source": [
        "#Question 11/23. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (e.g., Iris dataset)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "print(\"Original data shape (X):\", X.shape)\n",
        "print(\"Original data shape (y):\", y.shape)\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 14. How can you find correlation between variables in Python?\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    'Feature1': [10, 20, 30, 40, 50],\n",
        "    'Feature2': [2, 4, 6, 8, 10], # Positively correlated\n",
        "    'Feature3': [50, 40, 30, 20, 10], # Negatively correlated\n",
        "    'Feature4': [1, 5, 2, 8, 3] # Weakly correlated\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# Calculate the correlation matrix for all numerical columns\n",
        "correlation_matrix = df.corr()\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# To find correlation between two specific columns\n",
        "corr_feature1_feature2 = df['Feature1'].corr(df['Feature2'])\n",
        "print(f\"\\nCorrelation between Feature1 and Feature2: {corr_feature1_feature2:.2f}\")\n",
        "\n",
        "corr_feature1_feature3 = df['Feature1'].corr(df['Feature3'])\n",
        "print(f\"Correlation between Feature1 and Feature3: {corr_feature1_feature3:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZjVX51CUSop",
        "outputId": "9e540830-5c8b-401f-eb73-4a520adc9468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample DataFrame:\n",
            "   Feature1  Feature2  Feature3  Feature4\n",
            "0        10         2        50         1\n",
            "1        20         4        40         5\n",
            "2        30         6        30         2\n",
            "3        40         8        20         8\n",
            "4        50        10        10         3\n",
            "\n",
            "Correlation Matrix:\n",
            "          Feature1  Feature2  Feature3  Feature4\n",
            "Feature1  1.000000  1.000000 -1.000000  0.398862\n",
            "Feature2  1.000000  1.000000 -1.000000  0.398862\n",
            "Feature3 -1.000000 -1.000000  1.000000 -0.398862\n",
            "Feature4  0.398862  0.398862 -0.398862  1.000000\n",
            "\n",
            "Correlation between Feature1 and Feature2: 1.00\n",
            "Correlation between Feature1 and Feature3: -1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 17. What does model.fit() do? What arguments must be given?\n",
        "#Question 18. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Create a synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10 # 100 samples, 1 feature\n",
        "y = 2 * X + 1 + np.random.randn(100, 1) * 2 # y = 2x + 1 + noise\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Linear Regression model instance\n",
        "model = LinearRegression()\n",
        "\n",
        "# model.fit(X_train, y_train) - Training the model\n",
        "# X_train: Training features (independent variables)\n",
        "# y_train: Target variable for training (dependent variable)\n",
        "print(\"Training the model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "print(f\"\\nModel coefficients: {model.coef_}\")\n",
        "print(f\"Model intercept: {model.intercept_}\")\n",
        "\n",
        "# model.predict(X_test) - Making predictions\n",
        "# X_test: New, unseen features for which predictions are desired\n",
        "print(\"\\nMaking predictions on the test set...\")\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"\\nMean Squared Error on test set: {mse:.2f}\")\n",
        "\n",
        "# Example of predicting a single new data point\n",
        "new_data_point = np.array([[7.5]])\n",
        "predicted_value = model.predict(new_data_point)\n",
        "print(f\"Prediction for new data point {new_data_point[0][0]}: {predicted_value[0][0]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqOVCPz0Uapk",
        "outputId": "f6b96aea-ad31-4c31-9e60-7b49db2e1f03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Model training complete.\n",
            "\n",
            "Model coefficients: [[1.9066758]]\n",
            "Model intercept: [1.23580112]\n",
            "\n",
            "Making predictions on the test set...\n",
            "Predictions made.\n",
            "\n",
            "Mean Squared Error on test set: 2.52\n",
            "Prediction for new data point 7.5: 15.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 21. How do we perform scaling in Python?\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "# Load a sample dataset\n",
        "diabetes = load_diabetes()\n",
        "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "\n",
        "print(\"Original data (first 5 rows and a few columns):\")\n",
        "print(X.head())\n",
        "print(\"\\nDescriptive statistics before scaling:\")\n",
        "print(X.describe().loc[['mean', 'std', 'min', 'max']])\n",
        "\n",
        "# 1. Using StandardScaler (Z-score normalization)\n",
        "# Scales features to have a mean of 0 and standard deviation of 1\n",
        "scaler_standard = StandardScaler()\n",
        "X_scaled_standard = scaler_standard.fit_transform(X)\n",
        "X_scaled_standard_df = pd.DataFrame(X_scaled_standard, columns=X.columns)\n",
        "\n",
        "print(\"\\nData after StandardScaler (first 5 rows and a few columns):\")\n",
        "print(X_scaled_standard_df.head())\n",
        "print(\"\\nDescriptive statistics after StandardScaler:\")\n",
        "print(X_scaled_standard_df.describe().loc[['mean', 'std', 'min', 'max']])\n",
        "\n",
        "# 2. Using MinMaxScaler\n",
        "# Scales features to a specified range, typically 0 to 1\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_scaled_minmax = scaler_minmax.fit_transform(X)\n",
        "X_scaled_minmax_df = pd.DataFrame(X_scaled_minmax, columns=X.columns)\n",
        "\n",
        "print(\"\\nData after MinMaxScaler (first 5 rows and a few columns):\")\n",
        "print(X_scaled_minmax_df.head())\n",
        "print(\"\\nDescriptive statistics after MinMaxScaler:\")\n",
        "print(X_scaled_minmax_df.describe().loc[['mean', 'std', 'min', 'max']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln-l_5GWUnP7",
        "outputId": "5be68eeb-7ab6-41c4-8eaf-c698bf226158"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data (first 5 rows and a few columns):\n",
            "        age       sex       bmi        bp        s1        s2        s3  \\\n",
            "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
            "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
            "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
            "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
            "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
            "\n",
            "         s4        s5        s6  \n",
            "0 -0.002592  0.019907 -0.017646  \n",
            "1 -0.039493 -0.068332 -0.092204  \n",
            "2 -0.002592  0.002861 -0.025930  \n",
            "3  0.034309  0.022688 -0.009362  \n",
            "4 -0.002592 -0.031988 -0.046641  \n",
            "\n",
            "Descriptive statistics before scaling:\n",
            "               age           sex           bmi            bp            s1  \\\n",
            "mean -2.511817e-19  1.230790e-17 -2.245564e-16 -4.797570e-17 -1.381499e-17   \n",
            "std   4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
            "min  -1.072256e-01 -4.464164e-02 -9.027530e-02 -1.123988e-01 -1.267807e-01   \n",
            "max   1.107267e-01  5.068012e-02  1.705552e-01  1.320436e-01  1.539137e-01   \n",
            "\n",
            "                s2            s3            s4            s5            s6  \n",
            "mean  3.918434e-17 -5.777179e-18 -9.042540e-18  9.293722e-17  1.130318e-17  \n",
            "std   4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  \n",
            "min  -1.156131e-01 -1.023071e-01 -7.639450e-02 -1.260971e-01 -1.377672e-01  \n",
            "max   1.987880e-01  1.811791e-01  1.852344e-01  1.335973e-01  1.356118e-01  \n",
            "\n",
            "Data after StandardScaler (first 5 rows and a few columns):\n",
            "        age       sex       bmi        bp        s1        s2        s3  \\\n",
            "0  0.800500  1.065488  1.297088  0.459841 -0.929746 -0.732065 -0.912451   \n",
            "1 -0.039567 -0.938537 -1.082180 -0.553505 -0.177624 -0.402886  1.564414   \n",
            "2  1.793307  1.065488  0.934533 -0.119214 -0.958674 -0.718897 -0.680245   \n",
            "3 -1.872441 -0.938537 -0.243771 -0.770650  0.256292  0.525397 -0.757647   \n",
            "4  0.113172 -0.938537 -0.764944  0.459841  0.082726  0.327890  0.171178   \n",
            "\n",
            "         s4        s5        s6  \n",
            "0 -0.054499  0.418531 -0.370989  \n",
            "1 -0.830301 -1.436589 -1.938479  \n",
            "2 -0.054499  0.060156 -0.545154  \n",
            "3  0.721302  0.476983 -0.196823  \n",
            "4 -0.054499 -0.672502 -0.980568  \n",
            "\n",
            "Descriptive statistics after StandardScaler:\n",
            "               age           sex           bmi            bp            s1  \\\n",
            "mean -8.037814e-18  2.290777e-16  2.009453e-17 -1.607563e-17  8.037814e-18   \n",
            "std   1.001133e+00  1.001133e+00  1.001133e+00  1.001133e+00  1.001133e+00   \n",
            "min  -2.254290e+00 -9.385367e-01 -1.897929e+00 -2.363050e+00 -2.665411e+00   \n",
            "max   2.327895e+00  1.065488e+00  3.585718e+00  2.776058e+00  3.235851e+00   \n",
            "\n",
            "                s2            s3            s4        s5            s6  \n",
            "mean  4.018907e-18 -4.018907e-18  2.330966e-16  0.000000 -4.018907e-17  \n",
            "std   1.001133e+00  1.001133e+00  1.001133e+00  1.001133  1.001133e+00  \n",
            "min  -2.430626e+00 -2.150883e+00 -1.606102e+00 -2.651040 -2.896390e+00  \n",
            "max   4.179278e+00  3.809072e+00  3.894331e+00  2.808722  2.851075e+00  \n",
            "\n",
            "Data after MinMaxScaler (first 5 rows and a few columns):\n",
            "        age  sex       bmi        bp        s1        s2        s3        s4  \\\n",
            "0  0.666667  1.0  0.582645  0.549296  0.294118  0.256972  0.207792  0.282087   \n",
            "1  0.483333  0.0  0.148760  0.352113  0.421569  0.306773  0.623377  0.141044   \n",
            "2  0.883333  1.0  0.516529  0.436620  0.289216  0.258964  0.246753  0.282087   \n",
            "3  0.083333  0.0  0.301653  0.309859  0.495098  0.447211  0.233766  0.423131   \n",
            "4  0.516667  0.0  0.206612  0.549296  0.465686  0.417331  0.389610  0.282087   \n",
            "\n",
            "         s5        s6  \n",
            "0  0.562217  0.439394  \n",
            "1  0.222437  0.166667  \n",
            "2  0.496578  0.409091  \n",
            "3  0.572923  0.469697  \n",
            "4  0.362385  0.333333  \n",
            "\n",
            "Descriptive statistics after MinMaxScaler:\n",
            "           age       sex       bmi        bp        s1        s2        s3  \\\n",
            "mean  0.491968  0.468326  0.346107  0.459817  0.451668  0.367725  0.360889   \n",
            "std   0.218484  0.499561  0.182567  0.194807  0.169647  0.151460  0.167977   \n",
            "min   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "max   1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
            "\n",
            "            s4        s5        s6  \n",
            "mean  0.291996  0.485560  0.503942  \n",
            "std   0.182010  0.183366  0.174187  \n",
            "min   0.000000  0.000000  0.000000  \n",
            "max   1.000000  1.000000  1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 24. Explain data encoding? (Coding for One-Hot and Label Encoding)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Create a sample DataFrame with categorical data\n",
        "data = {\n",
        "    'City': ['New York', 'Paris', 'London', 'New York', 'London', 'Paris'],\n",
        "    'Weather': ['Sunny', 'Cloudy', 'Rainy', 'Sunny', 'Rainy', 'Cloudy'],\n",
        "    'Temperature': [25, 18, 12, 27, 10, 19]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# --- 1. Label Encoding ---\n",
        "# Best for ordinal categorical data (where order matters) or when few categories\n",
        "# Or for target variable encoding\n",
        "le = LabelEncoder()\n",
        "df['City_LabelEncoded'] = le.fit_transform(df['City'])\n",
        "print(\"\\nDataFrame after Label Encoding 'City':\")\n",
        "print(df[['City', 'City_LabelEncoded']])\n",
        "print(\"Classes learned by LabelEncoder for 'City':\", le.classes_)\n",
        "\n",
        "\n",
        "# --- 2. One-Hot Encoding ---\n",
        "# Best for nominal categorical data (where order does NOT matter)\n",
        "# Creates new binary (0 or 1) columns for each category\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse_output=False to get a dense array\n",
        "city_encoded = ohe.fit_transform(df[['City']]) # Note the double brackets for DataFrame input\n",
        "city_encoded_df = pd.DataFrame(city_encoded, columns=ohe.get_feature_names_out(['City']))\n",
        "\n",
        "# Combine with original DataFrame\n",
        "df_ohe = pd.concat([df, city_encoded_df], axis=1)\n",
        "\n",
        "print(\"\\nDataFrame after One-Hot Encoding 'City':\")\n",
        "print(df_ohe[['City'] + list(ohe.get_feature_names_out(['City']))])\n",
        "\n",
        "# Using pandas get_dummies for simpler One-Hot Encoding (often preferred)\n",
        "df_dummies = pd.get_dummies(df, columns=['Weather'], dtype=int)\n",
        "print(\"\\nDataFrame after One-Hot Encoding 'Weather' using pd.get_dummies:\")\n",
        "print(df_dummies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwF2aqb9UajK",
        "outputId": "6bcb5478-a795-43a3-e6c7-09e74317477e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "       City Weather  Temperature\n",
            "0  New York   Sunny           25\n",
            "1     Paris  Cloudy           18\n",
            "2    London   Rainy           12\n",
            "3  New York   Sunny           27\n",
            "4    London   Rainy           10\n",
            "5     Paris  Cloudy           19\n",
            "\n",
            "DataFrame after Label Encoding 'City':\n",
            "       City  City_LabelEncoded\n",
            "0  New York                  1\n",
            "1     Paris                  2\n",
            "2    London                  0\n",
            "3  New York                  1\n",
            "4    London                  0\n",
            "5     Paris                  2\n",
            "Classes learned by LabelEncoder for 'City': ['London' 'New York' 'Paris']\n",
            "\n",
            "DataFrame after One-Hot Encoding 'City':\n",
            "       City  City_London  City_New York  City_Paris\n",
            "0  New York          0.0            1.0         0.0\n",
            "1     Paris          0.0            0.0         1.0\n",
            "2    London          1.0            0.0         0.0\n",
            "3  New York          0.0            1.0         0.0\n",
            "4    London          1.0            0.0         0.0\n",
            "5     Paris          0.0            0.0         1.0\n",
            "\n",
            "DataFrame after One-Hot Encoding 'Weather' using pd.get_dummies:\n",
            "       City  Temperature  City_LabelEncoded  Weather_Cloudy  Weather_Rainy  \\\n",
            "0  New York           25                  1               0              0   \n",
            "1     Paris           18                  2               1              0   \n",
            "2    London           12                  0               0              1   \n",
            "3  New York           27                  1               0              0   \n",
            "4    London           10                  0               0              1   \n",
            "5     Paris           19                  2               1              0   \n",
            "\n",
            "   Weather_Sunny  \n",
            "0              1  \n",
            "1              0  \n",
            "2              0  \n",
            "3              1  \n",
            "4              0  \n",
            "5              0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AhlhD1wUaaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTzdpLQVUaWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}